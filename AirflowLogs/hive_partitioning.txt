*** Reading local file: /home/vedant/airflow/logs/210940125053_MiniProjectDag/hive_partitioning/2022-02-02T05:58:22.174914+00:00/1.log
[2022-02-02, 11:29:23 UTC] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: 210940125053_MiniProjectDag.hive_partitioning manual__2022-02-02T05:58:22.174914+00:00 [queued]>
[2022-02-02, 11:29:23 UTC] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: 210940125053_MiniProjectDag.hive_partitioning manual__2022-02-02T05:58:22.174914+00:00 [queued]>
[2022-02-02, 11:29:23 UTC] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-02-02, 11:29:23 UTC] {taskinstance.py:1239} INFO - Starting attempt 1 of 1
[2022-02-02, 11:29:23 UTC] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-02-02, 11:29:23 UTC] {taskinstance.py:1259} INFO - Executing <Task(BashOperator): hive_partitioning> on 2022-02-02 05:58:22.174914+00:00
[2022-02-02, 11:29:23 UTC] {standard_task_runner.py:52} INFO - Started process 24422 to run task
[2022-02-02, 11:29:23 UTC] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', '210940125053_MiniProjectDag', 'hive_partitioning', 'manual__2022-02-02T05:58:22.174914+00:00', '--job-id', '160', '--raw', '--subdir', 'DAGS_FOLDER/airflowDag.py', '--cfg-path', '/tmp/tmpz55kif_v', '--error-file', '/tmp/tmpk4pnawft']
[2022-02-02, 11:29:23 UTC] {standard_task_runner.py:77} INFO - Job 160: Subtask hive_partitioning
[2022-02-02, 11:29:23 UTC] {logging_mixin.py:109} INFO - Running <TaskInstance: 210940125053_MiniProjectDag.hive_partitioning manual__2022-02-02T05:58:22.174914+00:00 [running]> on host vedant-HP-Pavilion-Laptop-15-cc1xx
[2022-02-02, 11:29:23 UTC] {taskinstance.py:1424} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=210940125053_MiniProjectDag
AIRFLOW_CTX_TASK_ID=hive_partitioning
AIRFLOW_CTX_EXECUTION_DATE=2022-02-02T05:58:22.174914+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-02-02T05:58:22.174914+00:00
[2022-02-02, 11:29:23 UTC] {subprocess.py:62} INFO - Tmp dir root location: 
 /tmp
[2022-02-02, 11:29:23 UTC] {subprocess.py:74} INFO - Running command: ['bash', '-c', 'spark-submit --master local Partitioning.py']
[2022-02-02, 11:29:23 UTC] {subprocess.py:85} INFO - Output:
[2022-02-02, 11:29:24 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:24 UTC WARN util.Utils: Your hostname, vedant-HP-Pavilion-Laptop-15-cc1xx resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface wlo1)
[2022-02-02, 11:29:24 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:24 UTC WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-02-02, 11:29:24 UTC] {subprocess.py:89} INFO - WARNING: An illegal reflective access operation has occurred
[2022-02-02, 11:29:24 UTC] {subprocess.py:89} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/vedant/DBDA_HOME/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-02-02, 11:29:24 UTC] {subprocess.py:89} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-02-02, 11:29:24 UTC] {subprocess.py:89} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-02-02, 11:29:24 UTC] {subprocess.py:89} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-02-02, 11:29:25 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:25 UTC INFO spark.SparkContext: Running Spark version 3.2.0
[2022-02-02, 11:29:25 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:25 UTC WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO resource.ResourceUtils: ==============================================================
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO resource.ResourceUtils: No custom resources configured for spark.driver.
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO resource.ResourceUtils: ==============================================================
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO spark.SparkContext: Submitted application: Python Spark SQL Hive
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO resource.ResourceProfile: Limiting resource is cpu
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO resource.ResourceProfileManager: Added ResourceProfile id: 0
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO spark.SecurityManager: Changing view acls to: vedant
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO spark.SecurityManager: Changing modify acls to: vedant
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO spark.SecurityManager: Changing view acls groups to:
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO spark.SecurityManager: Changing modify acls groups to:
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(vedant); groups with view permissions: Set(); users  with modify permissions: Set(vedant); groups with modify permissions: Set()
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO util.Utils: Successfully started service 'sparkDriver' on port 33943.
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO spark.SparkEnv: Registering MapOutputTracker
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO spark.SparkEnv: Registering BlockManagerMaster
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-40acab1f-9047-41a4-8138-142222d1c483
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO spark.SparkEnv: Registering OutputCommitCoordinator
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO util.log: Logging initialized @2831ms to org.sparkproject.jetty.util.log.Slf4jLog
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-Ubuntu-0ubuntu1.20.04
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO server.Server: Started @2928ms
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO server.AbstractConnector: Started ServerConnector@7727f697{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e6718e7{/jobs,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a66ed2{/jobs/json,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@134f972e{/jobs/job,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7296d02e{/jobs/job/json,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e2501b0{/stages,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64c6e3d1{/stages/json,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a220aac{/stages/stage,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ef45b14{/stages/stage/json,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c7faec5{/stages/pool,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1bca0c34{/stages/pool/json,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21974862{/storage,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4bf56d65{/storage/json,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e02cf09{/storage/rdd,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@251551a9{/storage/rdd/json,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@42a913c7{/environment,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54ddc9cd{/environment/json,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b663304{/executors,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@814b134{/executors/json,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a9de55e{/executors/threadDump,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19e61a40{/executors/threadDump/json,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34b7d267{/static,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10dc389d{/,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4f7a06a3{/api,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36b7e8ff{/jobs/job/kill,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e03122b{/stages/stage/kill,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:26 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:26 UTC INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.6:4040
[2022-02-02, 11:29:27 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:27 UTC INFO executor.Executor: Starting executor ID driver on host 192.168.1.6
[2022-02-02, 11:29:27 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:27 UTC INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37271.
[2022-02-02, 11:29:27 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:27 UTC INFO netty.NettyBlockTransferService: Server created on 192.168.1.6:37271
[2022-02-02, 11:29:27 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:27 UTC INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-02-02, 11:29:27 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:27 UTC INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.6, 37271, None)
[2022-02-02, 11:29:27 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:27 UTC INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.1.6:37271 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.1.6, 37271, None)
[2022-02-02, 11:29:27 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:27 UTC INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.6, 37271, None)
[2022-02-02, 11:29:27 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:27 UTC INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.6, 37271, None)
[2022-02-02, 11:29:27 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:27 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b588119{/metrics/json,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:27 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:27 UTC INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-02-02, 11:29:28 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:28 UTC INFO internal.SharedState: Warehouse path is 'hdfs://localhost:9000/user/hive/warehouse'.
[2022-02-02, 11:29:28 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:28 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6485e071{/SQL,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:28 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:28 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@100264d5{/SQL/json,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:28 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:28 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2021aa9d{/SQL/execution,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:28 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:28 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@645d7613{/SQL/execution/json,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:28 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:28 UTC INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4efc7c82{/static/sql,null,AVAILABLE,@Spark}
[2022-02-02, 11:29:29 UTC] {subprocess.py:89} INFO - ===============
[2022-02-02, 11:29:29 UTC] {subprocess.py:89} INFO - AppName: Python Spark SQL Hive
[2022-02-02, 11:29:29 UTC] {subprocess.py:89} INFO - Master: local
[2022-02-02, 11:29:29 UTC] {subprocess.py:89} INFO - ===============
[2022-02-02, 11:29:33 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:33 UTC INFO conf.HiveConf: Found configuration file null
[2022-02-02, 11:29:33 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:33 UTC INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
[2022-02-02, 11:29:33 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:33 UTC INFO client.HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is hdfs://localhost:9000/user/hive/warehouse
[2022-02-02, 11:29:33 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:33 UTC WARN conf.HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
[2022-02-02, 11:29:33 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:33 UTC WARN conf.HiveConf: HiveConf of name hive.stats.retries.wait does not exist
[2022-02-02, 11:29:33 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:33 UTC INFO metastore.HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
[2022-02-02, 11:29:33 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:33 UTC INFO metastore.ObjectStore: ObjectStore, initialize called
[2022-02-02, 11:29:33 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:33 UTC INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
[2022-02-02, 11:29:33 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:33 UTC INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored
[2022-02-02, 11:29:35 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:35 UTC INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
[2022-02-02, 11:29:36 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:36 UTC INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
[2022-02-02, 11:29:36 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:36 UTC INFO metastore.ObjectStore: Initialized ObjectStore
[2022-02-02, 11:29:36 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:36 UTC WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
[2022-02-02, 11:29:36 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:36 UTC WARN metastore.ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore vedant@127.0.1.1
[2022-02-02, 11:29:36 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:36 UTC INFO metastore.HiveMetaStore: Added admin role in metastore
[2022-02-02, 11:29:36 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:36 UTC INFO metastore.HiveMetaStore: Added public role in metastore
[2022-02-02, 11:29:36 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:36 UTC INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO metastore.HiveMetaStore: 0: get_database: default
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: default
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO metastore.HiveMetaStore: 0: get_database: global_temp
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: global_temp
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO metastore.HiveMetaStore: 0: create_database: Database(name:stage, description:, locationUri:hdfs://localhost:9000/user/hive/warehouse/stage.db, parameters:{}, ownerName:vedant)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=create_database: Database(name:stage, description:, locationUri:hdfs://localhost:9000/user/hive/warehouse/stage.db, parameters:{}, ownerName:vedant)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Database stage already exists)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:925)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at com.sun.proxy.$Proxy25.create_database(Unknown Source)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:725)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at com.sun.proxy.$Proxy26.createDatabase(Unknown Source)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:434)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createDatabase$1(HiveClientImpl.scala:345)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:343)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createDatabase$1(HiveExternalCatalog.scala:193)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:193)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createDatabase(ExternalCatalogWithListener.scala:47)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:251)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:83)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=staging
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=staging
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=staging
[2022-02-02, 11:29:37 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:37 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=staging
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=03a0dfe1-27ac-4f59-b0b0-ed69d4be2089, clientType=HIVECLI]
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO hive.metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: Cleaning up thread local RawStore...
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: Done cleaning up thread local RawStore
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:staging, dbName:stage, owner:vedant, createTime:1643781577, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:case_id, type:int, comment:null), FieldSchema(name:Hospital_code, type:string, comment:null), FieldSchema(name:Hospital_type_code, type:string, comment:null), FieldSchema(name:City_Code_Hospital, type:string, comment:null), FieldSchema(name:Hospital_region_code, type:string, comment:null), FieldSchema(name:Available_Extra_Rooms_in_Hospital, type:int, comment:null), FieldSchema(name:Department, type:string, comment:null), FieldSchema(name:Ward_Type, type:string, comment:null), FieldSchema(name:Ward_Facility_code, type:string, comment:null), FieldSchema(name:Bed_Grade, type:double, comment:null), FieldSchema(name:patientid, type:int, comment:null), FieldSchema(name:City_Code_Patient, type:string, comment:null), FieldSchema(name:Type_of_Admission, type:string, comment:null), FieldSchema(name:Severity_of_Illness, type:string, comment:null), FieldSchema(name:Visitors_with_Patient, type:int, comment:null), FieldSchema(name:Age_Range, type:string, comment:null), FieldSchema(name:Admission_Deposit, type:string, comment:null), FieldSchema(name:Stay, type:string, comment:null)], location:hdfs://localhost:9000/miniproject/persist, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=TRUE, spark.sql.sources.schema={"type":"struct","fields":[{"name":"case_id","type":"integer","nullable":true,"metadata":{}},{"name":"Hospital_code","type":"string","nullable":true,"metadata":{}},{"name":"Hospital_type_code","type":"string","nullable":true,"metadata":{}},{"name":"City_Code_Hospital","type":"string","nullable":true,"metadata":{}},{"name":"Hospital_region_code","type":"string","nullable":true,"metadata":{}},{"name":"Available_Extra_Rooms_in_Hospital","type":"integer","nullable":true,"metadata":{}},{"name":"Department","type":"string","nullable":true,"metadata":{}},{"name":"Ward_Type","type":"string","nullable":true,"metadata":{}},{"name":"Ward_Facility_code","type":"string","nullable":true,"metadata":{}},{"name":"Bed_Grade","type":"double","nullable":true,"metadata":{}},{"name":"patientid","type":"integer","nullable":true,"metadata":{}},{"name":"City_Code_Patient","type":"string","nullable":true,"metadata":{}},{"name":"Type_of_Admission","type":"string","nullable":true,"metadata":{}},{"name":"Severity_of_Illness","type":"string","nullable":true,"metadata":{}},{"name":"Visitors_with_Patient","type":"integer","nullable":true,"metadata":{}},{"name":"Age_Range","type":"string","nullable":true,"metadata":{}},{"name":"Admission_Deposit","type":"string","nullable":true,"metadata":{}},{"name":"Stay","type":"string","nullable":true,"metadata":{}}]}, spark.sql.create.version=3.2.0}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{vedant=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=create_table: Table(tableName:staging, dbName:stage, owner:vedant, createTime:1643781577, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:case_id, type:int, comment:null), FieldSchema(name:Hospital_code, type:string, comment:null), FieldSchema(name:Hospital_type_code, type:string, comment:null), FieldSchema(name:City_Code_Hospital, type:string, comment:null), FieldSchema(name:Hospital_region_code, type:string, comment:null), FieldSchema(name:Available_Extra_Rooms_in_Hospital, type:int, comment:null), FieldSchema(name:Department, type:string, comment:null), FieldSchema(name:Ward_Type, type:string, comment:null), FieldSchema(name:Ward_Facility_code, type:string, comment:null), FieldSchema(name:Bed_Grade, type:double, comment:null), FieldSchema(name:patientid, type:int, comment:null), FieldSchema(name:City_Code_Patient, type:string, comment:null), FieldSchema(name:Type_of_Admission, type:string, comment:null), FieldSchema(name:Severity_of_Illness, type:string, comment:null), FieldSchema(name:Visitors_with_Patient, type:int, comment:null), FieldSchema(name:Age_Range, type:string, comment:null), FieldSchema(name:Admission_Deposit, type:string, comment:null), FieldSchema(name:Stay, type:string, comment:null)], location:hdfs://localhost:9000/miniproject/persist, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=TRUE, spark.sql.sources.schema={"type":"struct","fields":[{"name":"case_id","type":"integer","nullable":true,"metadata":{}},{"name":"Hospital_code","type":"string","nullable":true,"metadata":{}},{"name":"Hospital_type_code","type":"string","nullable":true,"metadata":{}},{"name":"City_Code_Hospital","type":"string","nullable":true,"metadata":{}},{"name":"Hospital_region_code","type":"string","nullable":true,"metadata":{}},{"name":"Available_Extra_Rooms_in_Hospital","type":"integer","nullable":true,"metadata":{}},{"name":"Department","type":"string","nullable":true,"metadata":{}},{"name":"Ward_Type","type":"string","nullable":true,"metadata":{}},{"name":"Ward_Facility_code","type":"string","nullable":true,"metadata":{}},{"name":"Bed_Grade","type":"double","nullable":true,"metadata":{}},{"name":"patientid","type":"integer","nullable":true,"metadata":{}},{"name":"City_Code_Patient","type":"string","nullable":true,"metadata":{}},{"name":"Type_of_Admission","type":"string","nullable":true,"metadata":{}},{"name":"Severity_of_Illness","type":"string","nullable":true,"metadata":{}},{"name":"Visitors_with_Patient","type":"integer","nullable":true,"metadata":{}},{"name":"Age_Range","type":"string","nullable":true,"metadata":{}},{"name":"Admission_Deposit","type":"string","nullable":true,"metadata":{}},{"name":"Stay","type":"string","nullable":true,"metadata":{}}]}, spark.sql.create.version=3.2.0}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{vedant=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC WARN conf.HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC WARN conf.HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC WARN conf.HiveConf: HiveConf of name hive.stats.retries.wait does not exist
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.ObjectStore: ObjectStore, initialize called
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.ObjectStore: Initialized ObjectStore
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Table staging already exists)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1416)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1503)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at com.sun.proxy.$Proxy25.create_table_with_environment_context(Unknown Source)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at com.sun.proxy.$Proxy26.createTable(Unknown Source)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:553)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:551)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:287)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:376)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:167)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:department_partitioned, dbName:stage, owner:vedant, createTime:1643781578, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:case_id, type:int, comment:null), FieldSchema(name:Hospital_code, type:string, comment:null), FieldSchema(name:Hospital_type_code, type:string, comment:null), FieldSchema(name:City_Code_Hospital, type:string, comment:null), FieldSchema(name:Hospital_region_code, type:string, comment:null), FieldSchema(name:Available_Extra_Rooms_in_Hospital, type:int, comment:null), FieldSchema(name:Ward_Type, type:string, comment:null), FieldSchema(name:Ward_Facility_code, type:string, comment:null), FieldSchema(name:Bed_Grade, type:double, comment:null), FieldSchema(name:patientid, type:int, comment:null), FieldSchema(name:City_Code_Patient, type:string, comment:null), FieldSchema(name:Type_of_Admission, type:string, comment:null), FieldSchema(name:Severity_of_Illness, type:string, comment:null), FieldSchema(name:Visitors_with_Patient, type:int, comment:null), FieldSchema(name:Age_Range, type:string, comment:null), FieldSchema(name:Admission_Deposit, type:string, comment:null), FieldSchema(name:Stay, type:string, comment:null)], location:hdfs://localhost:9000/miniproject/partition/dept, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:Department, type:string, comment:null)], parameters:{EXTERNAL=TRUE, spark.sql.sources.schema.numPartCols=1, spark.sql.sources.schema.partCol.0=Department, spark.sql.sources.schema={"type":"struct","fields":[{"name":"case_id","type":"integer","nullable":true,"metadata":{}},{"name":"Hospital_code","type":"string","nullable":true,"metadata":{}},{"name":"Hospital_type_code","type":"string","nullable":true,"metadata":{}},{"name":"City_Code_Hospital","type":"string","nullable":true,"metadata":{}},{"name":"Hospital_region_code","type":"string","nullable":true,"metadata":{}},{"name":"Available_Extra_Rooms_in_Hospital","type":"integer","nullable":true,"metadata":{}},{"name":"Ward_Type","type":"string","nullable":true,"metadata":{}},{"name":"Ward_Facility_code","type":"string","nullable":true,"metadata":{}},{"name":"Bed_Grade","type":"double","nullable":true,"metadata":{}},{"name":"patientid","type":"integer","nullable":true,"metadata":{}},{"name":"City_Code_Patient","type":"string","nullable":true,"metadata":{}},{"name":"Type_of_Admission","type":"string","nullable":true,"metadata":{}},{"name":"Severity_of_Illness","type":"string","nullable":true,"metadata":{}},{"name":"Visitors_with_Patient","type":"integer","nullable":true,"metadata":{}},{"name":"Age_Range","type":"string","nullable":true,"metadata":{}},{"name":"Admission_Deposit","type":"string","nullable":true,"metadata":{}},{"name":"Stay","type":"string","nullable":true,"metadata":{}},{"name":"Department","type":"string","nullable":true,"metadata":{}}]}, spark.sql.create.version=3.2.0}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{vedant=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=create_table: Table(tableName:department_partitioned, dbName:stage, owner:vedant, createTime:1643781578, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:case_id, type:int, comment:null), FieldSchema(name:Hospital_code, type:string, comment:null), FieldSchema(name:Hospital_type_code, type:string, comment:null), FieldSchema(name:City_Code_Hospital, type:string, comment:null), FieldSchema(name:Hospital_region_code, type:string, comment:null), FieldSchema(name:Available_Extra_Rooms_in_Hospital, type:int, comment:null), FieldSchema(name:Ward_Type, type:string, comment:null), FieldSchema(name:Ward_Facility_code, type:string, comment:null), FieldSchema(name:Bed_Grade, type:double, comment:null), FieldSchema(name:patientid, type:int, comment:null), FieldSchema(name:City_Code_Patient, type:string, comment:null), FieldSchema(name:Type_of_Admission, type:string, comment:null), FieldSchema(name:Severity_of_Illness, type:string, comment:null), FieldSchema(name:Visitors_with_Patient, type:int, comment:null), FieldSchema(name:Age_Range, type:string, comment:null), FieldSchema(name:Admission_Deposit, type:string, comment:null), FieldSchema(name:Stay, type:string, comment:null)], location:hdfs://localhost:9000/miniproject/partition/dept, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:Department, type:string, comment:null)], parameters:{EXTERNAL=TRUE, spark.sql.sources.schema.numPartCols=1, spark.sql.sources.schema.partCol.0=Department, spark.sql.sources.schema={"type":"struct","fields":[{"name":"case_id","type":"integer","nullable":true,"metadata":{}},{"name":"Hospital_code","type":"string","nullable":true,"metadata":{}},{"name":"Hospital_type_code","type":"string","nullable":true,"metadata":{}},{"name":"City_Code_Hospital","type":"string","nullable":true,"metadata":{}},{"name":"Hospital_region_code","type":"string","nullable":true,"metadata":{}},{"name":"Available_Extra_Rooms_in_Hospital","type":"integer","nullable":true,"metadata":{}},{"name":"Ward_Type","type":"string","nullable":true,"metadata":{}},{"name":"Ward_Facility_code","type":"string","nullable":true,"metadata":{}},{"name":"Bed_Grade","type":"double","nullable":true,"metadata":{}},{"name":"patientid","type":"integer","nullable":true,"metadata":{}},{"name":"City_Code_Patient","type":"string","nullable":true,"metadata":{}},{"name":"Type_of_Admission","type":"string","nullable":true,"metadata":{}},{"name":"Severity_of_Illness","type":"string","nullable":true,"metadata":{}},{"name":"Visitors_with_Patient","type":"integer","nullable":true,"metadata":{}},{"name":"Age_Range","type":"string","nullable":true,"metadata":{}},{"name":"Admission_Deposit","type":"string","nullable":true,"metadata":{}},{"name":"Stay","type":"string","nullable":true,"metadata":{}},{"name":"Department","type":"string","nullable":true,"metadata":{}}]}, spark.sql.create.version=3.2.0}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{vedant=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Table department_partitioned already exists)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1416)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1503)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at com.sun.proxy.$Proxy25.create_table_with_environment_context(Unknown Source)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at com.sun.proxy.$Proxy26.createTable(Unknown Source)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:553)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:551)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:287)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:376)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:167)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=staging
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=staging
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=staging
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=staging
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:38 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:38 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO datasources.InMemoryFileIndex: It took 130 ms to list leaf files for 1 paths.
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO datasources.FileSourceStrategy: Pushed Filters:
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO datasources.FileSourceStrategy: Post-Scan Filters:
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO datasources.FileSourceStrategy: Output Data Schema: struct<case_id: int, Hospital_code: string, Hospital_type_code: string, City_Code_Hospital: string, Hospital_region_code: string ... 16 more fields>
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=department_partitioned
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO metastore.HiveMetaStore: 0: get_partitions_ps_with_auth : db=stage tbl=department_partitioned[]
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_partitions_ps_with_auth : db=stage tbl=department_partitioned[]
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-02-02, 11:29:39 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:39 UTC INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2022-02-02, 11:29:40 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:40 UTC INFO codegen.CodeGenerator: Code generated in 367.590664 ms
[2022-02-02, 11:29:40 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:40 UTC INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 194.3 KiB, free 434.2 MiB)
[2022-02-02, 11:29:40 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:40 UTC INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 434.2 MiB)
[2022-02-02, 11:29:40 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:40 UTC INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.6:37271 (size: 34.7 KiB, free: 434.4 MiB)
[2022-02-02, 11:29:40 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:40 UTC INFO spark.SparkContext: Created broadcast 0 from sql at NativeMethodAccessorImpl.java:0
[2022-02-02, 11:29:40 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:40 UTC INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 8522187 bytes, open cost is considered as scanning 4194304 bytes.
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO spark.SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO scheduler.DAGScheduler: Got job 0 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (sql at NativeMethodAccessorImpl.java:0)
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO scheduler.DAGScheduler: Parents of final stage: List()
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO scheduler.DAGScheduler: Missing parents: List()
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 233.3 KiB, free 433.9 MiB)
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 82.8 KiB, free 433.9 MiB)
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.6:37271 (size: 82.8 KiB, free: 434.3 MiB)
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1427
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.6, executor driver, partition 0, ANY, 4934 bytes) taskResourceAssignments Map()
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO codegen.CodeGenerator: Code generated in 77.39745 ms
[2022-02-02, 11:29:41 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:41 UTC INFO codegen.CodeGenerator: Code generated in 16.567876 ms
[2022-02-02, 11:29:42 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:42 UTC INFO codegen.CodeGenerator: Code generated in 26.214679 ms
[2022-02-02, 11:29:42 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:42 UTC INFO datasources.FileScanRDD: Reading File path: hdfs://localhost:9000/miniproject/persist/part-00000-75350aa3-7600-459e-a382-23fa6fc18d7c-c000.snappy.parquet, range: 0-4327883, partition values: [empty row]
[2022-02-02, 11:29:42 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:42 UTC INFO compress.CodecPool: Got brand-new decompressor [.snappy]
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO codegen.CodeGenerator: Code generated in 35.23653 ms
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO codegen.CodeGenerator: Code generated in 20.507746 ms
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO codegen.CodeGenerator: Code generated in 40.381801 ms
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO hadoop.ParquetOutputFormat: Validation is off
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO hadoop.ParquetOutputFormat: Parquet properties are:
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - Parquet page size to 1048576
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - Parquet dictionary page size to 1048576
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - Dictionary is true
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - Writer version is: PARQUET_1_0
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - Page size checking is: estimated
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - Min row count for page size check is: 100
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - Max row count for page size check is: 10000
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - Truncate length for column indexes is: 64
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - Truncate length for statistics min/max  is: 2147483647
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - Bloom filter enabled: false
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - Max Bloom filter size for a column is 1048576
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - Bloom filter expected number of distinct values are: null
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - Page row count limit to 20000
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - Writing page checksums is: on
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   "type" : "struct",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   "fields" : [ {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "case_id",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_code",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_type_code",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Hospital",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_region_code",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "Available_Extra_Rooms_in_Hospital",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Type",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Facility_code",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "Bed_Grade",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "double",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "patientid",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Patient",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "Type_of_Admission",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "Severity_of_Illness",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "Visitors_with_Patient",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "Age_Range",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "Admission_Deposit",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "name" : "Stay",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   } ]
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - and corresponding Parquet message type:
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - message spark_schema {
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional int32 case_id;
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional binary Hospital_code (STRING);
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional binary Hospital_type_code (STRING);
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Hospital (STRING);
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional binary Hospital_region_code (STRING);
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional int32 Available_Extra_Rooms_in_Hospital;
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional binary Ward_Type (STRING);
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional binary Ward_Facility_code (STRING);
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional double Bed_Grade;
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional int32 patientid;
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Patient (STRING);
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional binary Type_of_Admission (STRING);
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional binary Severity_of_Illness (STRING);
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional int32 Visitors_with_Patient;
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional binary Age_Range (STRING);
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional binary Admission_Deposit (STRING);
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO -   optional binary Stay (STRING);
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:44 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:44 UTC INFO compress.CodecPool: Got brand-new compressor [.snappy]
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:45 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:45 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:45 UTC INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:45 UTC INFO hadoop.ParquetOutputFormat: Validation is off
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:45 UTC INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:45 UTC INFO hadoop.ParquetOutputFormat: Parquet properties are:
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Parquet page size to 1048576
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Parquet dictionary page size to 1048576
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Dictionary is true
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Writer version is: PARQUET_1_0
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Page size checking is: estimated
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Min row count for page size check is: 100
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Max row count for page size check is: 10000
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Truncate length for column indexes is: 64
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Truncate length for statistics min/max  is: 2147483647
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Bloom filter enabled: false
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Max Bloom filter size for a column is 1048576
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Bloom filter expected number of distinct values are: null
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Page row count limit to 20000
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Writing page checksums is: on
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:45 UTC INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   "type" : "struct",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   "fields" : [ {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "case_id",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_code",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_type_code",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Hospital",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_region_code",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Available_Extra_Rooms_in_Hospital",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Type",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Facility_code",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Bed_Grade",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "double",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "patientid",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Patient",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Type_of_Admission",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Severity_of_Illness",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Visitors_with_Patient",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Age_Range",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Admission_Deposit",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Stay",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   } ]
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - and corresponding Parquet message type:
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - message spark_schema {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional int32 case_id;
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Hospital_code (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Hospital_type_code (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Hospital (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Hospital_region_code (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional int32 Available_Extra_Rooms_in_Hospital;
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Ward_Type (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Ward_Facility_code (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional double Bed_Grade;
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional int32 patientid;
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Patient (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Type_of_Admission (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Severity_of_Illness (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional int32 Visitors_with_Patient;
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Age_Range (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Admission_Deposit (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Stay (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:45 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:45 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:45 UTC INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:45 UTC INFO hadoop.ParquetOutputFormat: Validation is off
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:45 UTC INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:45 UTC INFO hadoop.ParquetOutputFormat: Parquet properties are:
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Parquet page size to 1048576
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Parquet dictionary page size to 1048576
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Dictionary is true
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Writer version is: PARQUET_1_0
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Page size checking is: estimated
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Min row count for page size check is: 100
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Max row count for page size check is: 10000
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Truncate length for column indexes is: 64
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Truncate length for statistics min/max  is: 2147483647
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Bloom filter enabled: false
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Max Bloom filter size for a column is 1048576
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Bloom filter expected number of distinct values are: null
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Page row count limit to 20000
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - Writing page checksums is: on
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:45 UTC INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   "type" : "struct",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   "fields" : [ {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "case_id",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_code",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_type_code",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Hospital",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_region_code",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Available_Extra_Rooms_in_Hospital",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Type",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Facility_code",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Bed_Grade",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "double",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "patientid",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Patient",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Type_of_Admission",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Severity_of_Illness",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Visitors_with_Patient",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Age_Range",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Admission_Deposit",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "name" : "Stay",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   } ]
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - and corresponding Parquet message type:
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - message spark_schema {
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional int32 case_id;
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Hospital_code (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Hospital_type_code (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Hospital (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Hospital_region_code (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional int32 Available_Extra_Rooms_in_Hospital;
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Ward_Type (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Ward_Facility_code (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional double Bed_Grade;
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional int32 patientid;
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Patient (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Type_of_Admission (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Severity_of_Illness (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional int32 Visitors_with_Patient;
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Age_Range (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Admission_Deposit (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO -   optional binary Stay (STRING);
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:45 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO hadoop.ParquetOutputFormat: Validation is off
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO hadoop.ParquetOutputFormat: Parquet properties are:
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Parquet page size to 1048576
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Parquet dictionary page size to 1048576
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Dictionary is true
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Writer version is: PARQUET_1_0
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Page size checking is: estimated
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Min row count for page size check is: 100
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Max row count for page size check is: 10000
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Truncate length for column indexes is: 64
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Truncate length for statistics min/max  is: 2147483647
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Bloom filter enabled: false
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Max Bloom filter size for a column is 1048576
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Bloom filter expected number of distinct values are: null
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Page row count limit to 20000
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Writing page checksums is: on
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   "type" : "struct",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   "fields" : [ {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "case_id",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_code",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_type_code",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Hospital",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_region_code",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Available_Extra_Rooms_in_Hospital",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Type",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Facility_code",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Bed_Grade",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "double",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "patientid",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Patient",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Type_of_Admission",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Severity_of_Illness",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Visitors_with_Patient",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Age_Range",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Admission_Deposit",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Stay",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   } ]
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - and corresponding Parquet message type:
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - message spark_schema {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional int32 case_id;
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Hospital_code (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Hospital_type_code (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Hospital (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Hospital_region_code (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional int32 Available_Extra_Rooms_in_Hospital;
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Ward_Type (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Ward_Facility_code (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional double Bed_Grade;
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional int32 patientid;
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Patient (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Type_of_Admission (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Severity_of_Illness (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional int32 Visitors_with_Patient;
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Age_Range (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Admission_Deposit (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Stay (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO hadoop.ParquetOutputFormat: Validation is off
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO hadoop.ParquetOutputFormat: Parquet properties are:
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Parquet page size to 1048576
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Parquet dictionary page size to 1048576
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Dictionary is true
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Writer version is: PARQUET_1_0
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Page size checking is: estimated
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Min row count for page size check is: 100
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Max row count for page size check is: 10000
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Truncate length for column indexes is: 64
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Truncate length for statistics min/max  is: 2147483647
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Bloom filter enabled: false
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Max Bloom filter size for a column is 1048576
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Bloom filter expected number of distinct values are: null
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Page row count limit to 20000
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - Writing page checksums is: on
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   "type" : "struct",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   "fields" : [ {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "case_id",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_code",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_type_code",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Hospital",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_region_code",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Available_Extra_Rooms_in_Hospital",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Type",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Facility_code",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Bed_Grade",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "double",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "patientid",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Patient",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Type_of_Admission",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Severity_of_Illness",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Visitors_with_Patient",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Age_Range",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Admission_Deposit",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "name" : "Stay",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   } ]
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - and corresponding Parquet message type:
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - message spark_schema {
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional int32 case_id;
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Hospital_code (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Hospital_type_code (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Hospital (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Hospital_region_code (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional int32 Available_Extra_Rooms_in_Hospital;
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Ward_Type (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Ward_Facility_code (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional double Bed_Grade;
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional int32 patientid;
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Patient (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Type_of_Admission (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Severity_of_Illness (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional int32 Visitors_with_Patient;
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Age_Range (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Admission_Deposit (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO -   optional binary Stay (STRING);
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO output.FileOutputCommitter: Saved output of task 'attempt_202202021129415421644806812146184_0000_m_000000_0' to hdfs://localhost:9000/miniproject/partition/dept/.spark-staging-72feaec6-0cdc-4082-bbe7-74268e8b427d/_temporary/0/task_202202021129415421644806812146184_0000_m_000000
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO mapred.SparkHadoopMapRedUtil: attempt_202202021129415421644806812146184_0000_m_000000_0: Committed
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 3836 bytes result sent to driver
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6065 ms on 192.168.1.6 (executor driver) (1/1)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO scheduler.DAGScheduler: ResultStage 0 (sql at NativeMethodAccessorImpl.java:0) finished in 6.264 s
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO scheduler.DAGScheduler: Job 0 finished: sql at NativeMethodAccessorImpl.java:0, took 6.332654 s
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO datasources.FileFormatWriter: Start to commit write Job cdd99d1c-3946-4d20-9caa-2009f82d89a1.
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO datasources.FileFormatWriter: Write Job cdd99d1c-3946-4d20-9caa-2009f82d89a1 committed. Elapsed time: 105 ms.
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO datasources.FileFormatWriter: Finished processing stats for write job cdd99d1c-3946-4d20-9caa-2009f82d89a1.
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: create_table: Table(tableName:severity_partitioned, dbName:stage, owner:vedant, createTime:1643781587, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:case_id, type:int, comment:null), FieldSchema(name:Hospital_code, type:string, comment:null), FieldSchema(name:Hospital_type_code, type:string, comment:null), FieldSchema(name:City_Code_Hospital, type:string, comment:null), FieldSchema(name:Hospital_region_code, type:string, comment:null), FieldSchema(name:Available_Extra_Rooms_in_Hospital, type:int, comment:null), FieldSchema(name:Department, type:string, comment:null), FieldSchema(name:Ward_Type, type:string, comment:null), FieldSchema(name:Ward_Facility_code, type:string, comment:null), FieldSchema(name:Bed_Grade, type:double, comment:null), FieldSchema(name:patientid, type:int, comment:null), FieldSchema(name:City_Code_Patient, type:string, comment:null), FieldSchema(name:Type_of_Admission, type:string, comment:null), FieldSchema(name:Visitors_with_Patient, type:int, comment:null), FieldSchema(name:Age_Range, type:string, comment:null), FieldSchema(name:Admission_Deposit, type:string, comment:null), FieldSchema(name:Stay, type:string, comment:null)], location:hdfs://localhost:9000/miniproject/partition/severity, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:Severity_of_Illness, type:string, comment:null)], parameters:{EXTERNAL=TRUE, spark.sql.sources.schema.numPartCols=1, spark.sql.sources.schema.partCol.0=Severity_of_Illness, spark.sql.sources.schema={"type":"struct","fields":[{"name":"case_id","type":"integer","nullable":true,"metadata":{}},{"name":"Hospital_code","type":"string","nullable":true,"metadata":{}},{"name":"Hospital_type_code","type":"string","nullable":true,"metadata":{}},{"name":"City_Code_Hospital","type":"string","nullable":true,"metadata":{}},{"name":"Hospital_region_code","type":"string","nullable":true,"metadata":{}},{"name":"Available_Extra_Rooms_in_Hospital","type":"integer","nullable":true,"metadata":{}},{"name":"Department","type":"string","nullable":true,"metadata":{}},{"name":"Ward_Type","type":"string","nullable":true,"metadata":{}},{"name":"Ward_Facility_code","type":"string","nullable":true,"metadata":{}},{"name":"Bed_Grade","type":"double","nullable":true,"metadata":{}},{"name":"patientid","type":"integer","nullable":true,"metadata":{}},{"name":"City_Code_Patient","type":"string","nullable":true,"metadata":{}},{"name":"Type_of_Admission","type":"string","nullable":true,"metadata":{}},{"name":"Visitors_with_Patient","type":"integer","nullable":true,"metadata":{}},{"name":"Age_Range","type":"string","nullable":true,"metadata":{}},{"name":"Admission_Deposit","type":"string","nullable":true,"metadata":{}},{"name":"Stay","type":"string","nullable":true,"metadata":{}},{"name":"Severity_of_Illness","type":"string","nullable":true,"metadata":{}}]}, spark.sql.create.version=3.2.0}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{vedant=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=create_table: Table(tableName:severity_partitioned, dbName:stage, owner:vedant, createTime:1643781587, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:case_id, type:int, comment:null), FieldSchema(name:Hospital_code, type:string, comment:null), FieldSchema(name:Hospital_type_code, type:string, comment:null), FieldSchema(name:City_Code_Hospital, type:string, comment:null), FieldSchema(name:Hospital_region_code, type:string, comment:null), FieldSchema(name:Available_Extra_Rooms_in_Hospital, type:int, comment:null), FieldSchema(name:Department, type:string, comment:null), FieldSchema(name:Ward_Type, type:string, comment:null), FieldSchema(name:Ward_Facility_code, type:string, comment:null), FieldSchema(name:Bed_Grade, type:double, comment:null), FieldSchema(name:patientid, type:int, comment:null), FieldSchema(name:City_Code_Patient, type:string, comment:null), FieldSchema(name:Type_of_Admission, type:string, comment:null), FieldSchema(name:Visitors_with_Patient, type:int, comment:null), FieldSchema(name:Age_Range, type:string, comment:null), FieldSchema(name:Admission_Deposit, type:string, comment:null), FieldSchema(name:Stay, type:string, comment:null)], location:hdfs://localhost:9000/miniproject/partition/severity, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:Severity_of_Illness, type:string, comment:null)], parameters:{EXTERNAL=TRUE, spark.sql.sources.schema.numPartCols=1, spark.sql.sources.schema.partCol.0=Severity_of_Illness, spark.sql.sources.schema={"type":"struct","fields":[{"name":"case_id","type":"integer","nullable":true,"metadata":{}},{"name":"Hospital_code","type":"string","nullable":true,"metadata":{}},{"name":"Hospital_type_code","type":"string","nullable":true,"metadata":{}},{"name":"City_Code_Hospital","type":"string","nullable":true,"metadata":{}},{"name":"Hospital_region_code","type":"string","nullable":true,"metadata":{}},{"name":"Available_Extra_Rooms_in_Hospital","type":"integer","nullable":true,"metadata":{}},{"name":"Department","type":"string","nullable":true,"metadata":{}},{"name":"Ward_Type","type":"string","nullable":true,"metadata":{}},{"name":"Ward_Facility_code","type":"string","nullable":true,"metadata":{}},{"name":"Bed_Grade","type":"double","nullable":true,"metadata":{}},{"name":"patientid","type":"integer","nullable":true,"metadata":{}},{"name":"City_Code_Patient","type":"string","nullable":true,"metadata":{}},{"name":"Type_of_Admission","type":"string","nullable":true,"metadata":{}},{"name":"Visitors_with_Patient","type":"integer","nullable":true,"metadata":{}},{"name":"Age_Range","type":"string","nullable":true,"metadata":{}},{"name":"Admission_Deposit","type":"string","nullable":true,"metadata":{}},{"name":"Stay","type":"string","nullable":true,"metadata":{}},{"name":"Severity_of_Illness","type":"string","nullable":true,"metadata":{}}]}, spark.sql.create.version=3.2.0}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{vedant=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:vedant, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Table severity_partitioned already exists)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1416)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1503)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at com.sun.proxy.$Proxy25.create_table_with_environment_context(Unknown Source)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at com.sun.proxy.$Proxy26.createTable(Unknown Source)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:553)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:551)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:287)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:376)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:167)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO DataNucleus.Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=staging
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=staging
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=staging
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=staging
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO datasources.FileSourceStrategy: Pushed Filters:
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO datasources.FileSourceStrategy: Post-Scan Filters:
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO datasources.FileSourceStrategy: Output Data Schema: struct<case_id: int, Hospital_code: string, Hospital_type_code: string, City_Code_Hospital: string, Hospital_region_code: string ... 16 more fields>
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_database: stage
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:47 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:47 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO metastore.HiveMetaStore: 0: get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_table : db=stage tbl=severity_partitioned
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO metastore.HiveMetaStore: 0: get_partitions_ps_with_auth : db=stage tbl=severity_partitioned[]
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO HiveMetaStore.audit: ugi=vedant	ip=unknown-ip-addr	cmd=get_partitions_ps_with_auth : db=stage tbl=severity_partitioned[]
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO codegen.CodeGenerator: Code generated in 68.130755 ms
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 194.3 KiB, free 433.7 MiB)
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 433.6 MiB)
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.6:37271 (size: 34.7 KiB, free: 434.3 MiB)
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO spark.SparkContext: Created broadcast 2 from sql at NativeMethodAccessorImpl.java:0
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 8522187 bytes, open cost is considered as scanning 4194304 bytes.
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO spark.SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO scheduler.DAGScheduler: Got job 1 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (sql at NativeMethodAccessorImpl.java:0)
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO scheduler.DAGScheduler: Parents of final stage: List()
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO scheduler.DAGScheduler: Missing parents: List()
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 233.7 KiB, free 433.4 MiB)
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.8 KiB, free 433.3 MiB)
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.6:37271 (size: 82.8 KiB, free: 434.2 MiB)
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1427
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.6, executor driver, partition 0, ANY, 4934 bytes) taskResourceAssignments Map()
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.6:37271 in memory (size: 82.8 KiB, free: 434.3 MiB)
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO codegen.CodeGenerator: Code generated in 59.33213 ms
[2022-02-02, 11:29:48 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:48 UTC INFO datasources.FileScanRDD: Reading File path: hdfs://localhost:9000/miniproject/persist/part-00000-75350aa3-7600-459e-a382-23fa6fc18d7c-c000.snappy.parquet, range: 0-4327883, partition values: [empty row]
[2022-02-02, 11:29:49 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:49 UTC INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.6:37271 in memory (size: 34.7 KiB, free: 434.3 MiB)
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO codegen.CodeGenerator: Code generated in 13.224449 ms
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO hadoop.ParquetOutputFormat: Validation is off
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO hadoop.ParquetOutputFormat: Parquet properties are:
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Parquet page size to 1048576
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Parquet dictionary page size to 1048576
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Dictionary is true
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Writer version is: PARQUET_1_0
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Page size checking is: estimated
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Min row count for page size check is: 100
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Max row count for page size check is: 10000
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Truncate length for column indexes is: 64
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Truncate length for statistics min/max  is: 2147483647
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Bloom filter enabled: false
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Max Bloom filter size for a column is 1048576
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Bloom filter expected number of distinct values are: null
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Page row count limit to 20000
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Writing page checksums is: on
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   "type" : "struct",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   "fields" : [ {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "case_id",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_code",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_type_code",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Hospital",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_region_code",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Available_Extra_Rooms_in_Hospital",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Department",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Type",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Facility_code",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Bed_Grade",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "double",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "patientid",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Patient",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Type_of_Admission",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Visitors_with_Patient",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Age_Range",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Admission_Deposit",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Stay",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   } ]
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - and corresponding Parquet message type:
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - message spark_schema {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional int32 case_id;
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Hospital_code (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Hospital_type_code (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Hospital (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Hospital_region_code (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional int32 Available_Extra_Rooms_in_Hospital;
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Department (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Ward_Type (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Ward_Facility_code (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional double Bed_Grade;
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional int32 patientid;
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Patient (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Type_of_Admission (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional int32 Visitors_with_Patient;
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Age_Range (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Admission_Deposit (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Stay (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO hadoop.ParquetOutputFormat: Validation is off
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO hadoop.ParquetOutputFormat: Parquet properties are:
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Parquet page size to 1048576
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Parquet dictionary page size to 1048576
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Dictionary is true
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Writer version is: PARQUET_1_0
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Page size checking is: estimated
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Min row count for page size check is: 100
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Max row count for page size check is: 10000
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Truncate length for column indexes is: 64
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Truncate length for statistics min/max  is: 2147483647
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Bloom filter enabled: false
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Max Bloom filter size for a column is 1048576
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Bloom filter expected number of distinct values are: null
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Page row count limit to 20000
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - Writing page checksums is: on
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:50 UTC INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   "type" : "struct",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   "fields" : [ {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "case_id",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_code",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_type_code",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Hospital",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_region_code",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Available_Extra_Rooms_in_Hospital",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Department",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Type",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Facility_code",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Bed_Grade",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "double",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "patientid",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Patient",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Type_of_Admission",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Visitors_with_Patient",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Age_Range",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Admission_Deposit",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "name" : "Stay",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   } ]
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - and corresponding Parquet message type:
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - message spark_schema {
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional int32 case_id;
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Hospital_code (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Hospital_type_code (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Hospital (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Hospital_region_code (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional int32 Available_Extra_Rooms_in_Hospital;
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Department (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Ward_Type (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Ward_Facility_code (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional double Bed_Grade;
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional int32 patientid;
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Patient (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Type_of_Admission (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional int32 Visitors_with_Patient;
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Age_Range (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Admission_Deposit (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO -   optional binary Stay (STRING);
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:50 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:51 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:51 UTC INFO codec.CodecConfig: Compression: SNAPPY
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:51 UTC INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:51 UTC INFO hadoop.ParquetOutputFormat: Validation is off
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:51 UTC INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:51 UTC INFO hadoop.ParquetOutputFormat: Parquet properties are:
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - Parquet page size to 1048576
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - Parquet dictionary page size to 1048576
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - Dictionary is true
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - Writer version is: PARQUET_1_0
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - Page size checking is: estimated
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - Min row count for page size check is: 100
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - Max row count for page size check is: 10000
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - Truncate length for column indexes is: 64
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - Truncate length for statistics min/max  is: 2147483647
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - Bloom filter enabled: false
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - Max Bloom filter size for a column is 1048576
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - Bloom filter expected number of distinct values are: null
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - Page row count limit to 20000
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - Writing page checksums is: on
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:51 UTC INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   "type" : "struct",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   "fields" : [ {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "case_id",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_code",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_type_code",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Hospital",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "Hospital_region_code",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "Available_Extra_Rooms_in_Hospital",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "Department",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Type",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "Ward_Facility_code",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "Bed_Grade",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "double",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "patientid",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "City_Code_Patient",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "Type_of_Admission",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "Visitors_with_Patient",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "integer",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "Age_Range",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "Admission_Deposit",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   }, {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "name" : "Stay",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "type" : "string",
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "nullable" : true,
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -     "metadata" : { }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   } ]
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - and corresponding Parquet message type:
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - message spark_schema {
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional int32 case_id;
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional binary Hospital_code (STRING);
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional binary Hospital_type_code (STRING);
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Hospital (STRING);
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional binary Hospital_region_code (STRING);
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional int32 Available_Extra_Rooms_in_Hospital;
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional binary Department (STRING);
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional binary Ward_Type (STRING);
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional binary Ward_Facility_code (STRING);
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional double Bed_Grade;
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional int32 patientid;
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional binary City_Code_Patient (STRING);
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional binary Type_of_Admission (STRING);
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional int32 Visitors_with_Patient;
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional binary Age_Range (STRING);
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional binary Admission_Deposit (STRING);
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO -   optional binary Stay (STRING);
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - }
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:51 UTC] {subprocess.py:89} INFO - 
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO output.FileOutputCommitter: Saved output of task 'attempt_202202021129485764075083790112796_0001_m_000000_1' to hdfs://localhost:9000/miniproject/partition/severity/.spark-staging-efd65d01-1c90-4e8b-a506-8267ef9b666b/_temporary/0/task_202202021129485764075083790112796_0001_m_000000
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO mapred.SparkHadoopMapRedUtil: attempt_202202021129485764075083790112796_0001_m_000000_1: Committed
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 3675 bytes result sent to driver
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3757 ms on 192.168.1.6 (executor driver) (1/1)
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO scheduler.DAGScheduler: ResultStage 1 (sql at NativeMethodAccessorImpl.java:0) finished in 3.892 s
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO scheduler.DAGScheduler: Job 1 finished: sql at NativeMethodAccessorImpl.java:0, took 3.898242 s
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO datasources.FileFormatWriter: Start to commit write Job 5d14a5f1-1c74-47b8-8a22-b19e74ffb157.
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO datasources.FileFormatWriter: Write Job 5d14a5f1-1c74-47b8-8a22-b19e74ffb157 committed. Elapsed time: 196 ms.
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO datasources.FileFormatWriter: Finished processing stats for write job 5d14a5f1-1c74-47b8-8a22-b19e74ffb157.
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO spark.SparkContext: Invoking stop() from shutdown hook
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO server.AbstractConnector: Stopped Spark@7727f697{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO ui.SparkUI: Stopped Spark web UI at http://192.168.1.6:4040
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO memory.MemoryStore: MemoryStore cleared
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO storage.BlockManager: BlockManager stopped
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO storage.BlockManagerMaster: BlockManagerMaster stopped
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO spark.SparkContext: Successfully stopped SparkContext
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO util.ShutdownHookManager: Shutdown hook called
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f502b63b-f9fc-4ea9-b0c9-8aa63d64ece5
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO util.ShutdownHookManager: Deleting directory /tmp/spark-87cb5c7d-7eb6-4488-8bf6-2388e69976d2
[2022-02-02, 11:29:52 UTC] {subprocess.py:89} INFO - 2022-02-02, 11:29:52 UTC INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f502b63b-f9fc-4ea9-b0c9-8aa63d64ece5/pyspark-94d047d5-e61c-455b-a7a7-60aa717dc176
[2022-02-02, 11:29:53 UTC] {subprocess.py:93} INFO - Command exited with return code 0
[2022-02-02, 11:29:53 UTC] {taskinstance.py:1267} INFO - Marking task as SUCCESS. dag_id=210940125053_MiniProjectDag, task_id=hive_partitioning, execution_date=20220202T055822, start_date=20220202T055923, end_date=20220202T055953
[2022-02-02, 11:29:53 UTC] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-02-02, 11:29:53 UTC] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check

